#!/usr/bin/env python3
"""
KOSHA ê°€ì´ë“œ PDF ì—…ë¡œë“œ ì‹œìŠ¤í…œ - ì»¤ë§¨ë“œë¼ì¸ ë²„ì „
GUI ì—†ì´ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥
"""

import argparse
import pdfplumber
import psycopg2
import json
import re
import os
from typing import Dict, Any, List
from datetime import datetime
from pathlib import Path


class KoshaGuideParser:
    def __init__(self, db_config: Dict[str, str]):
        self.db_config = db_config
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        print(f"ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘: {pdf_path}")
        try:
            with pdfplumber.open(pdf_path) as pdf:
                text = ""
                for i, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                    print(f"   í˜ì´ì§€ {i}/{len(pdf.pages)} ì²˜ë¦¬ ì™„ë£Œ", end='\r')
                print()  # ì¤„ë°”ê¿ˆ
                return text
        except Exception as e:
            raise Exception(f"PDF íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    def extract_metadata(self, text: str) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        print("ğŸ” ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        metadata = {}
        
        # ê°€ì´ë“œ ë²ˆí˜¸
        guide_patterns = [
            r'KOSHA\s+GUIDE\s*\n\s*([P|C|M|E|H|G|W]\s*-?\s*\d+\s*[â€“-]\s*\d{4})',
            r'([P|C|M|E|H|G|W]\s*-\s*\d+\s*â€“\s*\d{4})',
        ]
        
        for pattern in guide_patterns:
            guide_match = re.search(pattern, text, re.IGNORECASE)
            if guide_match:
                metadata["guide_number"] = guide_match.group(1).strip().replace('â€“', '-').replace(' ', '')
                print(f"   âœ“ ê°€ì´ë“œ ë²ˆí˜¸: {metadata['guide_number']}")
                break
        
        # ì œëª© ì¶”ì¶œ
        title_patterns = [
            r'[â€“-]\s*\d{4}\s*\n\s*(.+?)\s*\n',
            r'GUIDE\s*\n\s*[P|C|M|E|H|G|W]\s*-\s*\d+\s*[â€“-]\s*\d{4}\s*\n\s*(.+?)\s*\n',
        ]
        
        for pattern in title_patterns:
            title_match = re.search(pattern, text)
            if title_match:
                title = title_match.group(1).strip()
                title = ' '.join(title.split())
                metadata["title"] = title
                print(f"   âœ“ ì œëª©: {title}")
                break
        
        # ë°œí–‰ì¼
        date_match = re.search(r'(\d{4})\s*\.\s*(\d{1,2})\s*\.', text)
        if date_match:
            year = date_match.group(1)
            month = date_match.group(2).zfill(2)
            metadata["publication_date"] = f"{year}-{month}"
            print(f"   âœ“ ë°œí–‰ì¼: {metadata['publication_date']}")
        
        # ë°œí–‰ê¸°ê´€
        metadata["publisher"] = "í•œêµ­ì‚°ì—…ì•ˆì „ë³´ê±´ê³µë‹¨"
        
        # ì‘ì„±ì ë° ê°œì •ì
        authors = []
        author_patterns = [r'ì‘ì„±ì:\s*(.+)', r'ê°œì •ì:\s*(.+)']
        for pattern in author_patterns:
            matches = re.findall(pattern, text)
            authors.extend([m.strip() for m in matches if m.strip()])
        
        if authors:
            metadata["authors"] = authors
            print(f"   âœ“ ì‘ì„±ì/ê°œì •ì: {len(authors)}ëª…")
        
        # ì œê°œì • ê²½ê³¼
        revision_history = []
        revision_match = re.search(r'ì œÂ·ê°œì • ê²½ê³¼\s*\n(.*?)(?=Â¡|ê´€ë ¨ ê·œê²©|$)', text, re.DOTALL)
        if revision_match:
            revision_text = revision_match.group(1)
            revisions = re.findall(r'-\s*(.+)', revision_text)
            revision_history = [rev.strip() for rev in revisions if rev.strip() and len(rev.strip()) > 5]
        
        if revision_history:
            metadata["revision_history"] = revision_history
            print(f"   âœ“ ê°œì • ì´ë ¥: {len(revision_history)}ê±´")
        
        # ê´€ë ¨ ê·œê²©
        related_standards = []
        standards_match = re.search(r'ê´€ë ¨ ê·œê²© ë° ìë£Œ\s*\n(.*?)(?=Â¡|ê¸°ìˆ ì§€ì¹¨ì˜|$)', text, re.DOTALL)
        if standards_match:
            standards_text = standards_match.group(1)
            standards = re.findall(r'-\s*(.+)', standards_text)
            related_standards = [std.strip() for std in standards if std.strip() and len(std.strip()) > 5]
        
        if related_standards:
            metadata["related_standards"] = related_standards
            print(f"   âœ“ ê´€ë ¨ ê·œê²©: {len(related_standards)}ê°œ")
        
        return metadata
    
    def extract_subsections(self, content: str) -> List[Dict[str, Any]]:
        """í•˜ìœ„ ì„¹ì…˜ ì¶”ì¶œ"""
        subsections = []
        
        # íŒ¨í„´ 1: ìˆ«ì.ìˆ«ì í˜•ì‹
        decimal_pattern = r'(\d+\.\d+)\s+([^\n]+)'
        decimal_matches = re.finditer(decimal_pattern, content)
        
        for match in decimal_matches:
            number = match.group(1)
            title = match.group(2).strip()
            
            if len(title) > 3:
                subsections.append({
                    "number": number,
                    "title": title[:150]
                })
        
        # íŒ¨í„´ 2: ê´„í˜¸ ë²ˆí˜¸ í˜•ì‹
        if not subsections:
            paren_patterns = [
                r'\((\d+)\)\s*([^\n]+)',
                r'\(([ê°€-í£])\)\s*([^\n]+)',
            ]
            
            for pattern in paren_patterns:
                paren_matches = re.finditer(pattern, content)
                for match in paren_matches:
                    number = match.group(1)
                    text = match.group(2).strip()
                    
                    if len(text) > 3:
                        subsections.append({
                            "number": f"({number})",
                            "content": text[:200]
                        })
                
                if subsections:
                    break
        
        return subsections
    
    def extract_sections(self, text: str) -> List[Dict[str, Any]]:
        """ë³¸ë¬¸ ì„¹ì…˜ ì¶”ì¶œ"""
        print("ğŸ“‘ ì„¹ì…˜ êµ¬ì¡° ë¶„ì„ ì¤‘...")
        sections = []
        
        section_pattern = r'\n(\d+)\.\s+([^\n]+)\n(.*?)(?=\n\d+\.\s+[^\n]+\n|<ë³„ì§€|$)'
        matches = re.finditer(section_pattern, text, re.DOTALL)
        
        for match in matches:
            section_num = match.group(1)
            section_title = match.group(2).strip()
            section_content = match.group(3).strip()
            
            if len(section_content) < 10:
                continue
            
            subsections = self.extract_subsections(section_content)
            
            section_data = {
                "number": section_num,
                "title": section_title,
                "content": section_content if len(section_content) < 500 else section_content[:500] + "...",
                "subsections": subsections
            }
            
            sections.append(section_data)
            print(f"   âœ“ ì„¹ì…˜ {section_num}: {section_title} (í•˜ìœ„ì„¹ì…˜: {len(subsections)}ê°œ)")
        
        return sections
    
    def extract_forms(self, text: str) -> List[Dict[str, Any]]:
        """ë³„ì§€ ì„œì‹ ì¶”ì¶œ"""
        forms = []
        
        form_pattern = r'<ë³„ì§€\s*ì„œì‹\s*(\d+)>\s*\n\s*([^\n]+)'
        matches = re.finditer(form_pattern, text)
        
        for match in matches:
            form_num = match.group(1)
            form_title = match.group(2).strip()
            
            forms.append({
                "form_number": form_num,
                "title": form_title
            })
        
        if forms:
            print(f"ğŸ“‹ ë³„ì§€ ì„œì‹: {len(forms)}ê°œ ë°œê²¬")
            for form in forms:
                print(f"   âœ“ ë³„ì§€ ì„œì‹ {form['form_number']}: {form['title']}")
        
        return forms
    
    def parse_kosha_guide(self, text: str) -> Dict[str, Any]:
        """KOSHA ê°€ì´ë“œ í…ìŠ¤íŠ¸ë¥¼ JSONìœ¼ë¡œ íŒŒì‹±"""
        
        metadata = self.extract_metadata(text)
        sections = self.extract_sections(text)
        forms = self.extract_forms(text)
        
        return {
            "document_id": metadata.get("guide_number", "UNKNOWN"),
            "title": metadata.get("title", "ì œëª© ì—†ìŒ"),
            "metadata": metadata,
            "sections": sections,
            "forms": forms
        }
    
    def save_to_database(self, parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()
            
            # ì¤‘ë³µ ì²´í¬
            cur.execute(
                "SELECT id, title FROM kosha_guide WHERE document_id = %s",
                (parsed_data["document_id"],)
            )
            existing = cur.fetchone()
            
            if existing:
                # ì—…ë°ì´íŠ¸
                print(f"   â„¹ï¸  ê¸°ì¡´ ë¬¸ì„œ ë°œê²¬ (ID: {existing[0]})")
                cur.execute("""
                    UPDATE kosha_guide 
                    SET title = %s,
                        content = %s,
                        updated_at = NOW()
                    WHERE document_id = %s
                    RETURNING id, document_id, title
                """, (
                    parsed_data["title"],
                    json.dumps(parsed_data, ensure_ascii=False),
                    parsed_data["document_id"]
                ))
                result = cur.fetchone()
                message = f"ê¸°ì¡´ ë¬¸ì„œ(ID: {existing[0]})ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."
            else:
                # ìƒˆë¡œ ìƒì„±
                print(f"   â„¹ï¸  ìƒˆ ë¬¸ì„œ ìƒì„±")
                cur.execute("""
                    INSERT INTO kosha_guide (document_id, title, content)
                    VALUES (%s, %s, %s)
                    RETURNING id, document_id, title
                """, (
                    parsed_data["document_id"],
                    parsed_data["title"],
                    json.dumps(parsed_data, ensure_ascii=False)
                ))
                result = cur.fetchone()
                message = "ìƒˆ ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
            
            conn.commit()
            cur.close()
            conn.close()
            
            print(f"   âœ… {message}")
            
            return {
                "id": result[0],
                "document_id": result[1],
                "title": result[2],
                "sections_count": len(parsed_data.get("sections", [])),
                "forms_count": len(parsed_data.get("forms", [])),
                "message": message
            }
        
        except psycopg2.Error as e:
            raise Exception(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
        except Exception as e:
            raise Exception(f"ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def process_file(self, pdf_path: str, output_json: str = None) -> Dict[str, Any]:
        """PDF íŒŒì¼ ì²˜ë¦¬"""
        print("=" * 80)
        print(f"KOSHA ê°€ì´ë“œ PDF íŒŒì‹± ì‹œì‘")
        print("=" * 80)
        print()
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = self.extract_text_from_pdf(pdf_path)
        print(f"   ì´ {len(text):,} ë¬¸ì ì¶”ì¶œ\n")
        
        # 2. íŒŒì‹±
        parsed_data = self.parse_kosha_guide(text)
        
        # 3. JSON íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
        if output_json:
            print(f"\nğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì¤‘: {output_json}")
            with open(output_json, 'w', encoding='utf-8') as f:
                json.dump(parsed_data, f, ensure_ascii=False, indent=2)
            print(f"   âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ")
        
        # 4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        result = self.save_to_database(parsed_data)
        
        # 5. ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ì²˜ë¦¬ ì™„ë£Œ!")
        print("=" * 80)
        print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
        print(f"   - ë°ì´í„°ë² ì´ìŠ¤ ID: {result['id']}")
        print(f"   - ë¬¸ì„œ ID: {result['document_id']}")
        print(f"   - ì œëª©: {result['title']}")
        print(f"   - ì„¹ì…˜ ìˆ˜: {result['sections_count']}ê°œ")
        print(f"   - ë³„ì§€ ì„œì‹: {result['forms_count']}ê°œ")
        print(f"   - ìƒíƒœ: {result['message']}")
        print()
        
        return result


def main():
    parser = argparse.ArgumentParser(
        description='KOSHA ê°€ì´ë“œ PDFë¥¼ íŒŒì‹±í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
  python cli_kosha_uploader.py input.pdf
  
  # JSON íŒŒì¼ë„ í•¨ê»˜ ì €ì¥
  python cli_kosha_uploader.py input.pdf --output output.json
  
  # í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ì§€ì •
  python cli_kosha_uploader.py input.pdf --env-file .env.production
  
  # DB ì„¤ì • ì§ì ‘ ì§€ì •
  python cli_kosha_uploader.py input.pdf --db-host localhost --db-name kosha_db
        """
    )
    
    parser.add_argument('pdf_file', help='ì²˜ë¦¬í•  PDF íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('-o', '--output', help='ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--env-file', default='.env', help='í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: .env)')
    
    # DB ì„¤ì • (ëª…ë ¹ì¤„ ì¸ìë¡œë„ ì§€ì • ê°€ëŠ¥)
    parser.add_argument('--db-host', help='ë°ì´í„°ë² ì´ìŠ¤ í˜¸ìŠ¤íŠ¸')
    parser.add_argument('--db-port', type=int, help='ë°ì´í„°ë² ì´ìŠ¤ í¬íŠ¸')
    parser.add_argument('--db-name', help='ë°ì´í„°ë² ì´ìŠ¤ ì´ë¦„')
    parser.add_argument('--db-user', help='ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ì')
    parser.add_argument('--db-password', help='ë°ì´í„°ë² ì´ìŠ¤ ë¹„ë°€ë²ˆí˜¸')
    
    args = parser.parse_args()
    
    # PDF íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(args.pdf_file):
        print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.pdf_file}")
        return 1
    
    # DB ì„¤ì • ë¡œë“œ
    db_config = {}
    
    # 1. .env íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
    if os.path.exists(args.env_file):
        try:
            from dotenv import load_dotenv
            load_dotenv(args.env_file)
            db_config = {
                'host': os.getenv('DB_HOST', 'localhost'),
                'port': int(os.getenv('DB_PORT', 5432)),
                'database': os.getenv('DB_NAME'),
                'user': os.getenv('DB_USER'),
                'password': os.getenv('DB_PASSWORD')
            }
        except ImportError:
            print("âš ï¸  ê²½ê³ : python-dotenvê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª…ë ¹ì¤„ ì¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    # 2. ëª…ë ¹ì¤„ ì¸ìë¡œ ë®ì–´ì“°ê¸°
    if args.db_host:
        db_config['host'] = args.db_host
    if args.db_port:
        db_config['port'] = args.db_port
    if args.db_name:
        db_config['database'] = args.db_name
    if args.db_user:
        db_config['user'] = args.db_user
    if args.db_password:
        db_config['password'] = args.db_password
    
    # DB ì„¤ì • ê²€ì¦
    required_keys = ['host', 'database', 'user', 'password']
    missing_keys = [key for key in required_keys if not db_config.get(key)]
    
    if missing_keys:
        print(f"âŒ ì˜¤ë¥˜: ë‹¤ìŒ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing_keys)}")
        print(f"   .env íŒŒì¼ì„ ìƒì„±í•˜ê±°ë‚˜ ëª…ë ¹ì¤„ ì¸ìë¡œ ì§€ì •í•˜ì„¸ìš”.")
        return 1
    
    # ì²˜ë¦¬ ì‹œì‘
    try:
        uploader = KoshaGuideParser(db_config)
        uploader.process_file(args.pdf_file, args.output)
        return 0
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main())